"""
Script para verificar si TensorFlow puede acceder a la GPU.
Muestra informaci√≥n sobre los dispositivos disponibles y configuraci√≥n.

Uso:
    python check_gpu.py
"""

import tensorflow as tf
import sys

def check_gpu():
    """Verifica la disponibilidad y configuraci√≥n de GPU."""
    print("=" * 60)
    print("üîç VERIFICACI√ìN DE GPU PARA TENSORFLOW")
    print("=" * 60)
    
    # Versi√≥n de TensorFlow
    print(f"\nüì¶ TensorFlow versi√≥n: {tf.__version__}")
    
    # Verificar si TensorFlow fue compilado con soporte CUDA
    print(f"\nüîß Compilado con CUDA: {tf.test.is_built_with_cuda()}")
    
    # Listar dispositivos f√≠sicos
    print("\nüíª Dispositivos f√≠sicos disponibles:")
    physical_devices = tf.config.list_physical_devices()
    if not physical_devices:
        print("   ‚ùå No se encontraron dispositivos")
    else:
        for device in physical_devices:
            print(f"   - {device.device_type}: {device.name}")
    
    # Verificar GPUs disponibles
    print("\nüéÆ GPUs disponibles:")
    gpus = tf.config.list_physical_devices('GPU')
    if not gpus:
        print("   ‚ùå No se encontraron GPUs")
        print("\nüí° Para habilitar GPU necesitas:")
        print("   1. GPU NVIDIA con soporte CUDA")
        print("   2. Controladores NVIDIA actualizados")
        print("   3. CUDA Toolkit 11.8+ y cuDNN 8.6+")
        print("   4. TensorFlow con soporte GPU:")
        print("      pip uninstall tensorflow")
        print("      pip install tensorflow[and-cuda]==2.17.1")
        return False
    else:
        for i, gpu in enumerate(gpus):
            print(f"   ‚úÖ GPU {i}: {gpu.name}")
            try:
                # Obtener detalles de la GPU
                gpu_details = tf.config.experimental.get_device_details(gpu)
                if gpu_details:
                    print(f"      Detalles: {gpu_details}")
            except:
                pass
    
    # Test de GPU
    print("\nüß™ Realizando test de GPU...")
    try:
        with tf.device('/GPU:0'):
            # Crear tensores de prueba
            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
            c = tf.matmul(a, b)
            print(f"   ‚úÖ Operaci√≥n matricial en GPU exitosa")
            print(f"   Resultado shape: {c.shape}")
    except RuntimeError as e:
        print(f"   ‚ùå Error al usar GPU: {e}")
        return False
    
    # Configuraci√≥n de memoria
    print("\nüíæ Configuraci√≥n de memoria GPU:")
    for gpu in gpus:
        try:
            # Habilitar crecimiento de memoria (evita reservar toda la GPU)
            tf.config.experimental.set_memory_growth(gpu, True)
            print(f"   ‚úÖ Crecimiento de memoria habilitado para {gpu.name}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudo configurar memoria: {e}")
    
    # Informaci√≥n de asignaci√≥n de dispositivos
    print("\nüó∫Ô∏è  Dispositivos l√≥gicos:")
    logical_devices = tf.config.list_logical_devices()
    for device in logical_devices:
        print(f"   - {device.device_type}: {device.name}")
    
    print("\n" + "=" * 60)
    print("‚úÖ GPU CONFIGURADA CORRECTAMENTE")
    print("=" * 60)
    print("\nüí° TensorFlow usar√° GPU autom√°ticamente")
    print("   El reconocimiento facial ser√° ~10-50x m√°s r√°pido")
    
    return True

def show_cuda_info():
    """Muestra informaci√≥n adicional sobre CUDA si est√° disponible."""
    print("\n" + "=" * 60)
    print("üîß INFORMACI√ìN DE CUDA")
    print("=" * 60)
    
    try:
        import subprocess
        
        # Intentar obtener versi√≥n de nvidia-smi
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total,memory.free,utilization.gpu',
                               '--format=csv,noheader'], 
                              capture_output=True, text=True, timeout=5)
        
        if result.returncode == 0:
            print("\nüìä Informaci√≥n de GPU (nvidia-smi):")
            lines = result.stdout.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) >= 5:
                    print(f"\n   GPU {i}:")
                    print(f"   - Nombre: {parts[0]}")
                    print(f"   - Driver: {parts[1]}")
                    print(f"   - Memoria Total: {parts[2]}")
                    print(f"   - Memoria Libre: {parts[3]}")
                    print(f"   - Utilizaci√≥n: {parts[4]}")
        else:
            print("\n‚ö†Ô∏è  nvidia-smi no disponible")
            print("   Aseg√∫rate de tener los drivers NVIDIA instalados")
            
    except FileNotFoundError:
        print("\n‚ö†Ô∏è  nvidia-smi no encontrado")
        print("   Instala los controladores NVIDIA desde:")
        print("   https://www.nvidia.com/Download/index.aspx")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Error al obtener informaci√≥n de CUDA: {e}")

def benchmark_cpu_vs_gpu():
    """Compara rendimiento CPU vs GPU."""
    print("\n" + "=" * 60)
    print("‚ö° BENCHMARK: CPU vs GPU")
    print("=" * 60)
    
    import time
    
    # Crear datos de prueba
    size = 5000
    iterations = 10
    
    print(f"\nGenerando matrices {size}x{size} para benchmark...")
    a = tf.random.normal([size, size])
    b = tf.random.normal([size, size])
    
    # Test en CPU
    print("\nüñ•Ô∏è  Probando en CPU...")
    with tf.device('/CPU:0'):
        start = time.time()
        for _ in range(iterations):
            c = tf.matmul(a, b)
        cpu_time = (time.time() - start) / iterations
        print(f"   Tiempo promedio: {cpu_time*1000:.2f} ms")
    
    # Test en GPU (si est√° disponible)
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print("\nüéÆ Probando en GPU...")
        with tf.device('/GPU:0'):
            # Warmup
            _ = tf.matmul(a, b)
            
            start = time.time()
            for _ in range(iterations):
                c = tf.matmul(a, b)
            gpu_time = (time.time() - start) / iterations
            print(f"   Tiempo promedio: {gpu_time*1000:.2f} ms")
        
        speedup = cpu_time / gpu_time
        print(f"\nüöÄ Aceleraci√≥n GPU: {speedup:.2f}x m√°s r√°pido que CPU")
        
        if speedup < 2:
            print("\n‚ö†Ô∏è  La aceleraci√≥n es menor de lo esperado.")
            print("   Esto puede ser normal para operaciones peque√±as.")
            print("   En reconocimiento facial real la mejora ser√° mayor.")
    else:
        print("\n‚ùå No hay GPU disponible para comparar")

def main():
    """Funci√≥n principal."""
    # Verificar GPU
    gpu_available = check_gpu()
    
    # Mostrar informaci√≥n de CUDA
    show_cuda_info()
    
    # Benchmark si hay GPU
    if gpu_available:
        try:
            benchmark_cpu_vs_gpu()
        except Exception as e:
            print(f"\n‚ö†Ô∏è  No se pudo ejecutar benchmark: {e}")
    
    print("\n" + "=" * 60)
    if gpu_available:
        print("üéâ Sistema listo para usar GPU")
        print("   El reconocimiento facial usar√° CUDA autom√°ticamente")
    else:
        print("‚ÑπÔ∏è  Sistema funcionar√° con CPU")
        print("   Considera instalar soporte GPU para mejor rendimiento")
    print("=" * 60)

if __name__ == "__main__":
    main()
